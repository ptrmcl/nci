<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Website for Noise-Coded Illumination for Forensic & Photometric Analysis">
  <meta property="og:title" content="Noise-Coded Illumination for Forensic and Photometric Analysis"/>
  <meta property="og:description" content="A scene-level intervention for helping detect spatiotemporal manipulations of video by coding the light in an environment with noise."/>
  <meta property="og:url" content="peterfmichael.com/nci"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Noise-Coded Illumination for Forensic and Photometric Analysis">
  <meta name="twitter:description" content="A scene-level intervention for helping detect spatiotemporal manipulations of video by coding the light in an environment with noise.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="forensics, deepfakes">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Noise-Coded Illumination</title>
  <link rel="icon" type="image/x-icon" href="static/images/lamp.svg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Noise-Coded Illumination</h1>
            <h2 class="subtitle is-4 publication-subtitle">For Forensic & Photometric Video Analysis</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://peterfmichael.com" target="_blank">Peter F. Michael</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://zekunhao.com" target="_blank">Zekun Hao</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://sergebelongie.github.io" target="_blank">Serge Belongie</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://abedavis.com" target="_blank">Abe Davis</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Cornell University <sup>2</sup>Cornell Tech <sup>3</sup>University of Copenhagen<br>Transactions on Graphics (June 2025)</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3742892" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (Open Access)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Supplementary Website link -->
                  <span class="link-block">
                    <a href="https://boiled-mochi-bd3.notion.site/Supplemental-Material-edba45bcc1544bedb0db041279931186" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-camera"></i>
                    </span>
                    <span>Experiment Repository</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="publication-video">
        <!-- Youtube embed code here -->
        <iframe src="https://www.youtube.com/embed/UUa_CqjeYA8?si=UTazS_IAPAFTQlDL" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      <h2 class="subtitle has-text-centered">
        We introduce Noise-Coded Illumination (NCI), a scene-level intervention for helping detect spatiotemporal manipulations of video by coding the light in an environment with noise.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Problem</h2>
        <div class="content has-text-justified">
          <p>
            The proliferation of advanced tools for manipulating video has led to an arms race, pitting those who wish to sow disinformation against those who
            want to detect and expose it. Unfortunately, time favors the ill-intentioned in this race, with fake videos growing increasingly difficult to distinguish from real ones. 
            At the root of this trend is a fundamental advantage held by those manipulating media: equal access to a distribution of what we consider authentic (i.e., "natural") video. 
          </p>
          <h2 class="title is-3"><center>Our Contribution</center></h2>
          <p>
            We show how coding very subtle, noise-like modulations into the illumination of a scene can help combat this advantage by creating an information asymmetry that 
            favors verification. Our approach effectively adds a temporal watermark to any video recorded under coded illumination. However, rather than encoding a specific message, 
            this watermark encodes an image of the unmanipulated scene as it would appear lit only by the coded illumination. We show that even when an adversary knows that our technique 
            is being used, creating plausible coded fake video amounts to solving a second, more difficult version of the original adversarial content creation problem at an information disadvantage.
          </p>
          <h2 class="title is-3"><center>Target Application</center></h2>
          <p>Our work targets high-stakes settings like public events and interviews, where the content on display is a likely target for manipulation, and while the 
          illumination can be controlled, the cameras capturing video cannot.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="section" id="Method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <h3 class="title">Coding the Illumination in the Scene</h3>
    We experimented with two types of light sources, computer monitors for applications such as video conferencing and stage lights for larger-scale events.
    <br><br>
    <img align="right" src="static/images/LightImageInset.jpg" width="200px" height="200px"/>
    As with many lights, the <a target="_blank" href="https://www.amazon.com/dp/B0BSGYKJJ7?th=1"/>stage light</a> we purchased adopts the 
    <a target="_blank" href="https://www.nema.org/docs/default-source/standards-document-library/ansi-c137-1-2019-contents-and-scope.pdf"/> ANSI 0-10V dimming standard</a>, which we can use for injecting our time-varying illumination code. 
    However, we found that the dimming system has a low pass filter with a sub-1Hz cutoff, which prevented us from directly injecting our 12Hz code signal.
    We fix this by adjusting the filter components and bypassing its internal pulse-width-modulation (PWM) section, a simple modification for manufacturers to incorporate that gives us a bandwidth of
    over 100Hz. We use an ESP-32 microcontroller running our compiled C code to modulate the light with our code signal, with interactive control of the average brightness and signal amplitude.
    <h3 class="title">Detecting Temporal Manipulations</h3>
    <video poster="" id="tree" autoplay controls muted loop height="100%">
      <!-- Your video here -->
      <source src="static/videos/temporal.mp4"
      type="video/mp4">
    </video>
    <br><br>We can use local evidence of the code signal in a video to create alignment matrices that map each point in time in a video to a point in our code signal.
    Discontinuities in the alignment matrix indicate that a video has been temporally tampered. 
    Most interviews use a tri-camera setup, like the one shown above, which are especially vulnerable to temporal tampering, since an adversary can easily cut between cameras to 
    change the meaning of what was said, as shown below (view the original and edited videos and alignment matrices <a target="_blank" href="https://boiled-mochi-bd3.notion.site/Television-Interview-15c016299dfb4b36bf18423974d2457f">here</a>).
   <br><br>
   <figure>
   <img align="right" src="static/images/temporal_analysis.webp"/>
   <figcaption>In the original dialogue of our scene, the
    interviewee expresses concern about fake video in political campaigns (mid left). The maliciously edited version splices in footage from an earlier response
    given during a sound check that makes it look like the interviewee supports and encourages the use of fake video for spreading disinformation (mid right).
    Our recovered alignment matrix displays the original timing of each clip, which shows that the answers in the manipulated video came from footage recorded
    before the corresponding questions, indicating that they were maliciously taken out of context.</figcaption>
  </figure>
    <h3 class="title">Detecting Spatial Manipulations</h3>
    <video poster="" id="tree" autoplay controls muted loop height="100%">
      <!-- Your video here -->
      <source src="static/videos/spatial.mp4"
      type="video/mp4">
    </video>
    <br><br>By assigning N uncorrelated code signals to light sources in a scene, we can perform light source seperation into N components,
    each lit by the light sources corresponding to that code, as shown above. Typically, lighting is one of the hardest things to
    fake in a video. With our approach, an adversary needs to somehow recover the illumination codes, which our supplemental analysis suggests is difficult for general types of manipulation, and
    fake the lighting in N+1 images instead of just 1. We show an example below on analyzing a spatially tampered video from an uninformed adversary, meaning one
    that doesn't know our technique is being used.<br><br>

    <figure>
    <img src="static/images/CarolineDecompWManipPageWidthCodeShadow_comp.jpg"/>
    <figcaption>In code images from the fake video (bottom) we see several signs of manipulation. The added content does not
      appear to reflect coded light. Code images also make it easier to see shadows that are masked by other light sources in the original frame. The shadow that
      the person’s head casts from the LED lamp is barely visible in the original video due to an uncoded ceiling light. However, in code image 1, all other sources
      are removed, leaving this part of the shadow clearly visible. The similar code shadows for added content are missing from manipulated code images. We
      recommend zooming in to better view these results.</figcaption>
  </figure>

    <br><br><br><b>Check out our <a target="_blank" href="https://boiled-mochi-bd3.notion.site/Supplemental-Material-edba45bcc1544bedb0db041279931186">experiment repository</a> 
    and <a target="_blank" href="https://dl.acm.org/doi/suppl/10.1145/3742892/suppl_file/tog-24-0102-File002.pdf">supplemental material</a> for results on diverse scenes such as those recorded outdoors and with dark-skinned subjects, 
    as well as analysis of human flicker senstivity from coding light, adversarial attacks from informed adversaries, and the effects of real-world phenonema such as video compression.</b>
  </div>
</section>

<!-- Acks -->
<section class="section" id="Acks">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>We thank all those who volunteered to be subjects in our test scenes.
      This work was supported in part by an NDSEG fellowship to P.M., and the Pioneer Centre for AI, DNRF
      grant number P1.</p>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@article{noisecodedillumination,
author = {Michael, Peter and Hao, Zekun and Belongie, Serge and Davis, Abe},
title = {Noise-Coded Illumination for Forensic and Photometric Video Analysis},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3742892},
doi = {10.1145/3742892},
journal = {ACM Trans. Graph.},
month = jun,
articleno = {165},
numpages = {16},
keywords = {Video forensics, video manipulation, forgery detection, computational illumination}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
